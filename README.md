# LAMMPS

## Building the container images
_Note: this requires access to a machine with the Docker CLI available_

Clone this repo:
```
git clone https://github.com/alahiff/lammps.git
cd lammps
```
Build an image for `lmp_serial` with OpenMP support:
```
docker build -f Dockerfile-serial -t alahiff/lammps-serial-omp:latest .
```
Build an image for `lmp_mpi` with OpenMP support:
```
docker build -f Dockerfile-mpi -t alahiff/lammps-openmpi-omp:latest .
```
In the above of you should use your own Docker Hub user name of course. Then push to Docker Hub:
```
docker push alahiff/lammps-serial-omp:latest
docker push alahiff/lammps-openmpi-omp:latest
```

## Basic tests using Docker
_Note: this requires access to a machine with the Docker CLI available_

Running the benchmarks using Docker as a quick test:
```
wget https://github.com/lammps/lammps/archive/stable_12Dec2018.tar.gz
tar xzf stable_12Dec2018.tar.gz
cd lammps-stable_12Dec2018/bench
```
Any of the benchmark problems can now be run, for example LJ:
```
docker run -it --rm -v `pwd`:/work --workdir /work -e OMP_NUM_THREADS=2 alahiff/lammps-openmpi-omp:latest lmp_mpi -sf omp -in in.lj
```
or EAM:
```
docker run -it --rm -v `pwd`:/work --workdir /work -e OMP_NUM_THREADS=2 alahiff/lammps-openmpi-omp:latest lmp_mpi -sf omp -in in.eam
```

## Running on DiRAC
There are two options for running LAMMPS on DiRAC in unprivileged containers: Singularity and udocker.

### Using Singularity
Singularity is available by default on DiRAC. On a login node, pull the required container image, e.g.:
```
singularity pull docker://alahiff/lammps-openmpi-omp:latest
```
For this example the file `lammps-openmpi-omp-latest.simg` was created.

## Using udocker
First of all udocker needs to be installed by running:
```
curl https://raw.githubusercontent.com/indigo-dc/udocker/master/udocker.py > udocker
chmod u+rx ./udocker
./udocker install
mv udocker ~/bin/.
```
Now we can pull the image and create a container:
```
udocker pull alahiff/lammps-openmpi-omp 
udocker create --name=lammps alahiff/lammps-openmpi-omp
```

## Running using PROMINENCE
These examples make use of the PROMINENCE Command Line Interface. See https://prominence-eosc.github.io/docs/ for more information.

### Single node: MPI + OpenMP
Here we run one of the benchmark problems using 8 cores, with 4 MPI processes each running 2 OpenMP threads:
```
prominence create --name lammps-lj \
                  --cpus 8 \
                  --memory 8 \
                  --env OMP_NUM_THREADS=2 \
                  --artifact https://github.com/lammps/lammps/archive/stable_12Dec2018.tar.gz \
                  --workdir lammps-stable_12Dec2018/bench \
                  alahiff/lammps-openmpi-omp:latest \
                  "mpirun -np 4 lmp_mpi -sf omp -in in.lj"
```
> output
```
Job created with id 22387
```
Checking the status of the job
```
prominence list
```
> output
```
ID      NAME        CREATED               STATUS      ELAPSED      IMAGE                               CMD                                   
22387   lammps-lj   2019-06-04T14:30:36   completed   0+00:04:39   alahiff/lammps-openmpi-omp:latest   mpirun -np 4 lmp_mpi -sf omp -in in.lj
```
The standard output generated by the job can be seen by typing e.g.
```
prominence stdout 22387
```
> output
```
LAMMPS (12 Dec 2018)
  using 2 OpenMP thread(s) per MPI task
using multi-threaded neighbor list subroutines
Lattice spacing in x,y,z = 1.6796 1.6796 1.6796
Created orthogonal box = (0 0 0) to (33.5919 33.5919 33.5919)
  1 by 2 by 2 MPI processor grid
Created 32000 atoms
  Time spent = 0.00167628 secs
Last active /omp style is pair_style lj/cut/omp
...
```


### Multiple nodes: MPI + OpenMP
Here we run one of the benchmark problems using 8 cores in total, but this time with 4 nodes, each running a single MPI process which each run 2 OpenMP threads:
```
prominence create --name lammps-lj \
                  --cpus 2 \
                  --memory 2 \
                  --nodes 4 \
                  --procs-per-node 1 \
                  --openmpi \   
                  --env OMP_NUM_THREADS=2 \
                  --artifact https://github.com/lammps/lammps/archive/stable_12Dec2018.tar.gz \
                  --workdir lammps-stable_12Dec2018/bench \
                  alahiff/lammps-openmpi-omp:latest \
                  "lmp_mpi -sf omp -in in.lj"
```
